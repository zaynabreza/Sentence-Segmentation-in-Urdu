{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11_QtarKSjPf"
   },
   "source": [
    "### Sentence Segmentation in Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIhS278lWQOv"
   },
   "outputs": [],
   "source": [
    "## for installing UrduHack\n",
    "#pip install urduhack[tf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XChiPrRsSwjz"
   },
   "source": [
    "##### Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxjaBCgOWYKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('sent-test.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "f.close()\n",
    "text = passage[0][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segmentation using UrduHack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے', 'کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔']\n"
     ]
    }
   ],
   "source": [
    "import urduhack\n",
    "from urduhack.tokenization import sentence_tokenizer\n",
    "\n",
    "sentences = sentence_tokenizer(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gaElHuGS4Qb"
   },
   "source": [
    "#### Reading Segmented File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnPi90zhegAM"
   },
   "outputs": [],
   "source": [
    "with open('sent-segmented.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    segmented = list(reader)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_text = segmented[0][0]\n",
    "sents = seg_text.split('۔')\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urduhack.tokenization import word_tokenizer\n",
    "w_list = word_tokenizer(seg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عقل'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with the UrduHack Library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word Tokenizer\n",
    "   It divides whole words into two separate words as well at moments.\n",
    "#### 2. Sentence Tokenizer\n",
    "1. It has a very limited set of conjunctions and end of sentence words which lead to highly inaccurate sentence segmentation.\n",
    "2. There was no handling of the \"!\" sign and even with \"-\" and \"?\" signs, they were not handles in one go but rather separately leading to extra computation.\n",
    "3. There were some extra checks in place which I did not deem necessary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algortithm Implemented:\n",
    "##### Two global sets have been defined, one for Conjunctions and one for EndOFSentence words. These are much more extensive than the ones in the Urdu Hack library.\n",
    " Firstly, using the function \"splitandkeep\", the text is divided into sentences based on already existing instances of \"-\", \"?\" and \"!\". In this function, the words are tokenized using the split() function based on spaces and then whenever the  given delimeters are ecnountered, it is counted as a complete sentence and appended to the list of sentences.\n",
    " \n",
    "After this, each sentence in the list returned by splitandkeep is tokenized into words using .split() function on the basis of spaces. Each word encountered is added to a temporary string until such a word is encountered that is in the set of EndOfSentence words. If this happens, the algorithm checks that the next word should not exist in the Conjunctions set and that the next word should also not be ending in \"-\", \"?\" or \"!\". If all the above conditions are true then the temporary string is added to the list of finalsentences and the string is emptied for next sentence. There is also a check that is the encountered word is the last in the pre-made sentences (using splitandkeep) then also the temporarystring is appended to list of finalsentences and emptied for next sentence.\n",
    " \n",
    "The function returns a list of the finalsentences. As a preliminary test, after the function is called, a loop runs that appends each sentence to a string called finaltext, adding the \"-\" sign at the end of every sentence that does not already have \"?\", \"-\" or \"!\". Both the finaltext and the correct expected output are then printed.\n",
    "\n",
    "Accuracy is computed in the next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of Accuracy:\n",
    "Using the splitandkeep function, both the finaltext and the expected correct output are divided into lists of sentences based on occurence of \"-\", \"?\" and \"!\".\n",
    "\n",
    "Then using nested loop, for every sentence in the correctAnswer list, the list of sentences in answerToCheck (the computed answer) is iterated over and it is checked if the last four (less if the length of sentences is lesser) words in the correctAnswer sentence and the answerToCheck sentence are the same. If they are then it is considered a hit and the index is also saved for correctAnswer so that next time the iteration starts from the next sentence so that there is no duplication that the same computed sentence is matched with 2 or more correct sentences in the list and  to avoid extra computation\n",
    "\n",
    "At the end, accuracy is computed using the formula hits/total * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Your Sentence Segmentation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "My answer:\n",
      "\n",
      "\n",
      "ایلون مسک کا یہ بیان ان خبروں کے جواب میں آیا ہے جن کے مطابق چین کی فوج نے اپنے تمام سٹیشنز پر ٹیسلا پر پابندی عائد کر دی ہے۔ ان کاروں میں نصب کیے گئے کیمروں میں ڈیٹا کولیکشن کے بعد چین کی فوج نے سکیورٹی سے متعلق اپنے تحفظات کا اظہار کیا تھا۔ اس کمپنی کی گذشتہ برس کی ایک چوتھائی فروخت پر نظر دوڑائی جائے تو امریکہ کے بعد چین ٹیسلا کاروں کی سب سے بڑی مارکیٹ ہے۔ سنیچر کو ایلون مسک کا کہنا تھا کہ اگر کوئی کاروبار غیر ملکی حکومتوں کی جاسوسی کے لیے مامور کر دیا جائے تو پھر ایسے میں اس کمپنی پر اس کے بہت منفی اثرات مرتب ہو سکتے ہیں۔ ایلون مسک نے ویڈیو لنک کے ذریعے ایک بااثر چینی بزنس فورم سے خطاب کے دوران بتایا کہ کسی بھی معلومات کو رازداری میں رکھنے کے ہمارے لیے بہت فوائد ہیں۔ انھوں نے کہا کہ اگر ٹیسلا کی کاریں چین یا کہیں اور جاسوسی میں استعمال ہوئی تو ہم کمپنی بند کر دیں گے۔ طویل عرصے سے چین اور امریکہ کی تجارتی کمپنیوں کو ایک دوسرے کے ملک میں کام کرتے ہوئے کئی مسائل کا سامنا کرنا پڑتا ہے۔ گذشتہ کئی برس سے دنیا کی دو بڑی معیشتوں چین اور امریکہ کے باہمی تعلقات سرد مہری کا شکار ہیں۔ چین کی امریکی صدر جو بائیڈن کی انتظامیہ کے ساتھ اس ہفتے کے آغاز میں پہلی اعلی سطح کی براہ راست ملاقات میں سخت الفاظ کا تبادلہ بھی ہوا۔ ایلون مسک نے چین اور امریکہ کے درمیان بڑے پیمانے پر اعتماد کی فضا پیدا کرنے کی ضرورت پر زور دیا ہے۔ چینی ویڈیو پلیٹ فارم ٹک ٹاک کا حوالہ دیتے ہوئے انھوں نے مطالبہ کیا کہ کمپنیوں پر اس طرح کے تحفظات کو نہ بڑھایا جائے کہ وہ اپنی حکومتوں کے لیے جاسوسی کرتی ہیں۔ واضح رہے کہ گذشتہ برس امریکہ کے سابق صدر ڈونلڈ ٹرمپ نے ان تحفظات کی بنیاد پر کہ کمپنی چین کی حکومت کے ساتھ ڈیٹا شئیر کر سکتی ہے، ٹک ٹاک پر پابندی عائد کرنے کا عندیہ دیا تھا۔ ایلون مسک نے کہا کہ اگر یہ مان بھی لیا جائے کہ یہ جاسوسی ہو رہی ہے تو دوسرا ملک اس سے کیا حاصل کر لے گا اور اس کے لیے ایسی معلومات کس کام کی ہوں گی؟ ایلون مسک کی کاروں کی کمپنی نے سنہ 2018 میں شنگھائی میں فیکٹری لگانے کی منظوری حاصل کی تھی۔ یوں یہ پہلی غیر ملکی کمپنی بن گئی جس کا چین میں ہر لحاظ سے مکمل پلانٹ کام کرنا شروع ہو گیا۔ چین اس وقت کاروں کی سب سے بڑی مارکیٹ بن چکی ہے اور چینی حکومت الیکٹرک کاروں کو فروغ دے رہی ہے۔ طلب میں اس اضافے سے ٹیسلا نے گذشتہ سال 721 ملین ڈالر کا منافع کمایا۔ \n",
      "\n",
      "\n",
      "Actual answer:\n",
      "\n",
      "\n",
      "ایلون مسک کا یہ بیان ان خبروں کے جواب میں آیا ہے جن کے مطابق چین کی فوج نے اپنے تمام سٹیشنز پر ٹیسلا پر پابندی عائد کر دی ہے۔ ان کاروں میں نصب کیے گئے کیمروں میں ڈیٹا کولیکشن کے بعد چین کی فوج نے سکیورٹی سے متعلق اپنے تحفظات کا اظہار کیا تھا۔ اس کمپنی کی گذشتہ برس کی ایک چوتھائی فروخت پر نظر دوڑائی جائے تو امریکہ کے بعد چین ٹیسلا کاروں کی سب سے بڑی مارکیٹ ہے۔ سنیچر کو ایلون مسک کا کہنا تھا کہ اگر کوئی کاروبار غیر ملکی حکومتوں کی جاسوسی کے لیے مامور کر دیا جائے تو پھر ایسے میں اس کمپنی پر اس کے بہت منفی اثرات مرتب ہو سکتے ہیں۔ ایلون مسک نے ویڈیو لنک کے ذریعے ایک بااثر چینی بزنس فورم سے خطاب کے دوران بتایا کہ کسی بھی معلومات کو رازداری میں رکھنے کے ہمارے لیے بہت فوائد ہیں۔ انھوں نے کہا کہ اگر ٹیسلا کی کاریں چین یا کہیں اور جاسوسی میں استعمال ہوئی تو ہم کمپنی بند کر دیں گے۔ طویل عرصے سے چین اور امریکہ کی تجارتی کمپنیوں کو ایک دوسرے کے ملک میں کام کرتے ہوئے کئی مسائل کا سامنا کرنا پڑتا ہے۔ گذشتہ کئی برس سے دنیا کی دو بڑی معیشتوں چین اور امریکہ کے باہمی تعلقات سرد مہری کا شکار ہیں۔ چین کی امریکی صدر جو بائیڈن کی انتظامیہ کے ساتھ اس ہفتے کے آغاز میں پہلی اعلی سطح کی براہ راست ملاقات میں سخت الفاظ کا تبادلہ بھی ہوا۔ ایلون مسک نے چین اور امریکہ کے درمیان بڑے پیمانے پر اعتماد کی فضا پیدا کرنے کی ضرورت پر زور دیا ہے۔ چینی ویڈیو پلیٹ فارم ٹک ٹاک کا حوالہ دیتے ہوئے انھوں نے مطالبہ کیا کہ کمپنیوں پر اس طرح کے تحفظات کو نہ بڑھایا جائے کہ وہ اپنی حکومتوں کے لیے جاسوسی کرتی ہیں۔ واضح رہے کہ گذشتہ برس امریکہ کے سابق صدر ڈونلڈ ٹرمپ نے ان تحفظات کی بنیاد پر کہ کمپنی چین کی حکومت کے ساتھ ڈیٹا شئیر کر سکتی ہے، ٹک ٹاک پر پابندی عائد کرنے کا عندیہ دیا تھا۔ ایلون مسک نے کہا کہ اگر یہ مان بھی لیا جائے کہ یہ جاسوسی ہو رہی ہے تو دوسرا ملک اس سے کیا حاصل کر لے گا اور اس کے لیے ایسی معلومات کس کام کی ہوں گی؟ ایلون مسک کی کاروں کی کمپنی نے سنہ 2018 میں شنگھائی میں فیکٹری لگانے کی منظوری حاصل کی تھی۔ یوں یہ پہلی غیر ملکی کمپنی بن گئی جس کا چین میں ہر لحاظ سے مکمل پلانٹ کام کرنا شروع ہو گیا۔ چین اس وقت کاروں کی سب سے بڑی مارکیٹ بن چکی ہے اور چینی حکومت الیکٹرک کاروں کو فروغ دے رہی ہے۔ طلب میں اس اضافے سے ٹیسلا نے گذشتہ سال 721 ملین ڈالر کا منافع کمایا۔\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#with open('sent-test.txt', 'rt', encoding=\"utf-8\") as f:  #opens the sent-test file \n",
    "#with open('urdu-corpus.txt', 'rt', encoding=\"utf-8\") as f: #opens the urdu-corpus file\n",
    "with open('articleincorrect.txt', 'rt', encoding=\"utf-8\") as f: #opens the article-incorrect file\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "f.close()\n",
    "text = passage[0][0]\n",
    "\n",
    "import urduhack\n",
    "from urduhack.tokenization import sentence_tokenizer\n",
    "\n",
    "import urduhack\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#global set of conjunctions\n",
    "Conjunctions= ['جنہیں', 'جس', 'جن', 'جو', 'اور', 'اگر', 'اگرچہ', 'لیکن', 'مگر', 'پر', 'یا', 'تاہم', 'کہ', 'کر',\n",
    "                    'بقول','جیسے','جسے', 'گی', 'جاسکتا', 'نہیں', 'ہو','باوجود','حالانکہ', 'لہذا','اتنی','اتنا','ورنہ', 'بلکہ','تاکہ','تاکہ','جاتا','چونکہ', 'کیونکہ', 'تو', 'گے',]\n",
    "\n",
    "#global set of EndOfSentence words\n",
    "EndOfSentence = ['کیجیے', 'کیجئے', 'گئیں', 'تھیں', 'ہوں', 'خریدا', 'گے', 'ہونگے', 'گا', 'چاہیے', 'ہوئیں', 'گی',\n",
    "                   'خریدا','رکھتے', 'ہوا', 'گیا', 'گئی','گرے','لگے','لگی', 'لگا', 'کہا', 'سکے', 'جاسکے','ہوگیا','ہوگا','تھا', 'تھی', 'تھے', 'ہیں', 'ہے',\n",
    "                       ]\n",
    "\n",
    "\n",
    "\n",
    "def splitandkeep(text, d1, d2=\"\", d3=\"\"):\n",
    "    answer=[]\n",
    "    if not text:\n",
    "        return answer\n",
    "    words=text.split() #sentence is divided into a list of words on the basis of \" \"\n",
    "    temp=\"\" #empty temporary string\n",
    "    for current, w in enumerate(words):\n",
    "        #print(\"On word\", w)\n",
    "        if w[-1]==d1 or w[-1]==d2 or w[-1]==d3: #if any word ends in any of the delimeters given in the function parameter\n",
    "            temp+=w #add the word to the temporary string\n",
    "            answer.append(temp.strip()) #append the string to the list of sentences in answer\n",
    "            temp=\"\" #empty the string for the next sentence\n",
    "        elif (current+1==len(words)): #if it is the last word in the given sentence\n",
    "            temp+=w\n",
    "            answer.append(temp.strip())\n",
    "            temp=\"\"\n",
    "        else: #if sentence not ending on the word, just append it to the temp string\n",
    "            temp+=w+\" \"\n",
    "    return answer\n",
    "        \n",
    "        \n",
    "with open('articlecorrect.txt', 'rt', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    passage = list(reader)\n",
    "f.close()\n",
    "correcttexta = passage[0][0] #correct answer for articleincorrect\n",
    "\n",
    "#correct answer for sent-test\n",
    "correcttextb=\"عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے۔ کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا۔ میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا۔ ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو۔\"\n",
    "\n",
    "#correct answer for first passage in urdu-corpus\n",
    "correcttextc=\"گزشتہ کئی سالوں سے مختلف بحران آتے جاتے رہے لیکن حالیہ آٹا ، چینی سمیت دیگر بحران اچانک پید ا ہوئے اور ان پر جے آئی ٹی تشکیل دے دیں گئیں تاکہ عوام کو ریلیف دیا جاسکے۔ دوسری جانب بجلی ، گیس ، پانی سمیت دیگر بلوں میں کئی سو گنا اضافہ کردیا گیا ،صوبائی و وفاقی وزراء نے اپنے اپنے ایوانوں بحران کے ذمہ دار عناصر کو بے نقاب کرنے کے بجائے سب اچھاہے کی رپورٹ پیش کیں ۔ساتھ ہی اورنگزیب کی طرح جواب دیا، حقائق کی ’’قبر ذرا گہری کھودنا!‘‘۔ تاریخی گواہ ہے بقول ماہرین ، مبصرین اور صحافیوں کا کہنا اور لکھنا ہے کہ پگڑی بدل بھائی کی رسم کی آڑ میں نادر شاہ نے محمد شاہ رنگیلا سے کوہِ نور ہیرا حاصل کیا تھا۔ 12 مئی 1739 کی شام۔ دہلی میں زبردست چہل پہل، شاہجہان آباد میں چراغاں اور لال قلعے میں جشن کا سماں ہے۔ غریبوں میں شربت، پان اور کھانا تقسیم کیا جا رہا ہے، فقیروں، گداؤں کو جھولی بھر بھر کر روپے عطا ہو رہے ہیں۔ آج دربار میں ایرانی بادشاہ نادر شاہ کے سامنے مغلیہ سلطنت کے 13ویں تاجدار محمد شاہ بیٹھے ہیں، لیکن اس وقت ان کے سر پر شاہی تاج نہیں ہے، کیوں نادر شاہ نے ڈھائی ماہ قبل ان سے سلطنت چھین لی تھی۔ 56 دن دہلی میں رہنے کے بعد اب نادر شاہ کے واپس ایران لوٹنے کا وقت آ گیا ہے اور وہ ہندوستان کی باگ ڈور دوبارہ سے محمد شاہ کے حوالے کرنا چاہتا ہے۔نادر شاہ نے صدیوں سے جمع کردہ مغل خزانے میں جھاڑو پھیر دی ہے اور شہر کے تمام امرا و روسا کی جیبیں الٹا لی ہیں، لیکن اسے دہلی کی ایک طوائف نور بائی نے، جس کا ذکر آگے چل کر آئے گا، خفیہ طور پر بتا دیا ہے کہ یہ سب کچھ جو تم نے حاصل کیا ہے، وہ ایسی چیز کے آگے ہیچ ہے جسے محمد شاہ نے اپنی پگڑی میں چھپا رکھا ہے۔ نادر شاہ گھاگ سیاستدان اور گھاٹ گھاٹ کا پانی پیے ہوئے تھا۔ اس موقعے پر وہ چال چلی جسے نہلے پہ دہلا کہا جاتا ہے۔ اس نے محمد شاہ سے کہا، ’’ایران میں رسم چلی آتی ہے کہ بھائی خوشی کے موقعے پر آپس میں پگڑیاں بدل دیتے ہیں، آج سے ہم بھائی بھائی بن گئے ہیں، تو کیوں نہ اسی رسم کا اعادہ کیا جائے؟‘‘ محمد شاہ کے پاس سر جھکانے کے علاوہ کوئی چارہ نہیں تھا۔ نادر شاہ نے اپنی پگڑی اتار کر اس کے سر رکھی، اور اس کی پگڑی اپنے سر، اور یوں دنیا کا مشہور ترین ہیرا کوہِ نور ہندوستان سے نکل کر ایران پہنچ گیا۔رنگیلا بادشاہ)اس ہیرے کے مالک محمد شاہ اپنے پڑدادا اورنگزیب عالمگیر کے دورِ حکومت میں 1702 میں پیدا ہوئے تھے۔ ان کا پیدائشی نام تو روشن اختر تھا، تاہم 29 ستمبر 1719 کو بادشاہ گر سید برادران نے انھیں صرف 17 برس کی عمر میں سلطنت تیموریہ کے تخت پر بٹھانے کے بعد ابوالفتح نصیر الدین روشن اختر محمد شاہ کا خطاب دیا۔ خود ان کا تخلص ’’سدا رنگیلا‘‘ تھا۔ اتنا لمبا نام کون یاد رکھتا، چنانچہ عوام نے دونوں کو ملا کر محمد شاہ رنگیلا کر دیا اور وہ آج تک ہندوستان کے طول و عرض میں اسی نام سے جانے اور مانے جاتے ہیں۔اورنگزیب عالمگیر نے ہندوستان میں ایک خاص قسم کا کٹر اسلام نافذ کر رکھا تھا۔محمد شاہ کی پیدائش کے وقت اورنگزیب عالمگیر نے ہندوستان میں ایک خاص قسم کا کٹر اسلام نافذ کر رکھا تھا اس کا سب سے پہلا نشانہ وہ فنونِ لطیفہ بنے جن کے بارے میں تصور تھا کہ وہ اسلامی اصولوں سے مطابقت نہیں رکھتے۔ اس کی ایک دلچسپ مثال اطالوی سیاح نکولو منوچی نے لکھی ہے۔ وہ کہتے ہیں کہ اورنگزیبی دور میں جب موسیقی پر پابندی لگی تو گویوں اور موسیقاروں کی روٹی روزی بند ہو گئی۔ آخر تنگ آ کر ایک ہزار فنکاروں نے جمعے کے دن دہلی کی جامع مسجد سے ایک جلوس نکالا اور آلات موسیقی کو جنازوں کی شکل میں لے کر روتے پیٹتے گزرنے لگے۔ اورنگزیب نے دیکھا تو حیرت زدہ ہو کر پچھوایا، یہ کس کا جنازہ لیے جا رہے ہو جس کی خاطر اس قدر آہ و بکا کیا جا رہا ہے؟’’ انھوں نے کہا: ‘‘آپ نے موسیقی قتل کر دی ہے اسے دفنانے جا رہے ہیں۔ اورنگزیب نے جواب دیا، ’’قبر ذرا گہری کھودنا!‘‘طبیعیات کا اصول ہے کہ ہر عمل کا ردِ عمل ہوتا ہے۔ یہی اصول تاریخ اور انسانی معاشرت پر بھی لاگو ہوتا ہے کہ جس چیز کو جتنی سختی سے دبایا جائے، وہ اتنی ہی قوت سے ابھر کر سامنے آتی ہے۔ چنانچہ اورنگزیب کے بعد بھی یہی کچھ ہوا اور محمد شاہ کے دور میں وہ تمام فنون پوری آب و تاب سے سامنے آ گئے جو اس سے پہلے دب گئے تھے۔\"\n",
    "\n",
    "\n",
    "def getSentences(text):\n",
    "    finalsentences=[]\n",
    "    initialsentences=[]\n",
    "    initialsentences = splitandkeep(text, \"۔\",\"؟\", \"!\") #get initial list of sentences based on occurence of \"۔\",\"؟\", \"!\"\n",
    "    for line in initialsentences:\n",
    "        words=line.split() #tokenize into words based on \" \"\n",
    "        new=\"\" #empty temporary string\n",
    "        for current, w in enumerate(words):\n",
    "            if w in EndOfSentence and current+1 < len(words): #if word is an EndOfSentence word\n",
    "                if (words[current+1] not in Conjunctions) and (words[current+1][-1] != '؟') and (words[current+1][-1] != '۔') and (words[current+1][-1] != '!'):\n",
    "                #if next word is not a conjunction and not end of a sentence either (doesnt end in \"۔\",\"؟\", \"!\")   \n",
    "                    if words[current + 1] in [\"۔\", \"،\"]:\n",
    "                     #if next word is a comma or -   \n",
    "                        new+=w+\" \"+words[current+1]\n",
    "                        finalsentences.append(new.strip()) #append the temporary string to the finalsentences list\n",
    "                        new=\"\" #empty string for new sentence\n",
    "                    \n",
    "                    else:\n",
    "                        new+=w\n",
    "                        finalsentences.append(new.strip()) #append the temporary string to the finalsentences list\n",
    "                        new=\"\"#empty string for new sentence                        \n",
    "                else:\n",
    "                    new+=w+\" \" #word is not EndOfSentence. Just add to the temporary string\n",
    "                    \n",
    "            elif (current+1==len(words)): #if word is last word of sentence\n",
    "                new+=w\n",
    "                finalsentences.append(new.strip()) #append the temporary string to the finalsentences list\n",
    "                new=\"\" #empty string for new sentence    \n",
    "            else:\n",
    "                new+=w+\" \" #word is not EndOfSentence. Just add to the temporary string\n",
    "    return finalsentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convertToText(finalsentences):\n",
    "    finaltext=\"\"    \n",
    "    for f in final:\n",
    "        if (f[-1][-1] == \"۔\" or f[-1][-1] == \"؟\" or f[-1][-1] == \"!\"): #sentence already ending in - or \"? \" or \"!\"\n",
    "            finaltext+=f+\" \" # add sentence to string of final text\n",
    "        else:\n",
    "            finaltext+=f+\"۔ \" # add sentence to string of final text with a \"۔\"\n",
    "    return finaltext\n",
    "        \n",
    "        \n",
    "final=getSentences(text)\n",
    "finaltext=convertToText(final)\n",
    "print(\"\\n\")        \n",
    "print(\"My answer:\")\n",
    "print(\"\\n\")\n",
    "print(finaltext)\n",
    "print(\"\\n\")\n",
    "print(\"Actual answer:\")\n",
    "print(\"\\n\")\n",
    "print(correcttexta)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZ83jAJ3S-uR"
   },
   "source": [
    "#### Compute Accuracy and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "OyFZPRIGlWNy",
    "outputId": "ca1debd5-4bd9-4202-c6bc-3bca33ad4400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits:  17\n",
      "Total sentences:  17\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(finaltext, correcttext):\n",
    "\n",
    "    answerToCheck=splitandkeep(finaltext, \"۔\", \"؟\", \"!\") #split computed answer into sentences\n",
    "    correctAnswer=splitandkeep(correcttext, \"۔\", \"؟\", \"!\") #split actual answer into sentences\n",
    "\n",
    "    hits=0\n",
    "    miss=0\n",
    "    total=0\n",
    "\n",
    "    indexchecked=0\n",
    "\n",
    "    for lines in correctAnswer: #for every correct sentence\n",
    "        total+=1\n",
    "        for n in range (indexchecked, len(answerToCheck)): #iterate over computed sentences, starting from the last matched sentence so that none is duplicated\n",
    "            answerlines=answerToCheck[n]\n",
    "            correctwords=lines.split() #split into words\n",
    "            answerwords=answerlines.split() #split into words\n",
    "\n",
    "            if (len(correctwords)>=4 and len(answerwords)>=4): #if length of both sentences is greater or equal to four words\n",
    "                if (correctwords[-1]==answerwords[-1] and correctwords[-2]==answerwords[-2] and correctwords[-3]==answerwords[-3] and correctwords[-4]==answerwords[-4]):\n",
    "                #if last four words of both sentences are the same then   \n",
    "                    hits+=1 #increment hits\n",
    "                    indexchecked=n\n",
    "                    continue\n",
    "                else:\n",
    "                    miss+=1\n",
    "            elif (len(correctwords)==2 or len(answerwords)==2): #if either of the sentence being checked has only two words\n",
    "            #if last two words of both sentences are the same then \n",
    "                if (correctwords[-1]==answerwords[-1] and correctwords[-2]==answerwords[-2]):\n",
    "                    hits+=1\n",
    "                    indexchecked=n\n",
    "                    continue\n",
    "                else:\n",
    "                    miss+=1\n",
    "\n",
    "            elif (len(correctwords)==3 or len(answerwords)==3): #if either of the sentence being checked has only three words\n",
    "            #if last three words of both sentences are the same then \n",
    "                if (correctwords[-1]==answerwords[-1] and correctwords[-2]==answerwords[-2] and correctwords[-3]==answerwords[-3]):\n",
    "                    hits+=1\n",
    "                    indexchecked=n\n",
    "                    continue\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    \n",
    "    return hits\n",
    "        \n",
    "       \n",
    "            \n",
    "hits=getAccuracy(finaltext, correcttexta) \n",
    "total=len(splitandkeep(correcttexta, \"۔\", \"؟\", \"!\"))\n",
    "\n",
    "print(\"Total hits: \", hits)\n",
    "print(\"Total sentences: \", total)        \n",
    "print(\"Accuracy: \", (hits/total)*100, \"%\") #compute accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
